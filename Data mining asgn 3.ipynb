{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.parsing.preprocessing import remove_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import gensim \n",
    "from gensim.models import Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    \"Read the contents of FILENAME and return as a string.\"\n",
    "    infile = open(filename) # windows users should use codecs.open after importing codecs\n",
    "    contents = infile.read()\n",
    "    infile.close()\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = 'shakespeare.txt'\n",
    "text = open(path_to_file, 'r').read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = remove_stopwords(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['mr','mrs','one','two','said','I','and','the','you','that','to','but','i','me','let','so','of','for','it','what','sir','o','a','is','as','my']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcount = {}\n",
    "for word in text.lower().split():\n",
    "    word = word.replace(\".\",\"\")\n",
    "    word = word.replace(\",\",\"\")\n",
    "    word = word.replace(\":\",\"\")\n",
    "    word = word.replace(\"\\\"\",\"\")\n",
    "    word = word.replace(\"!\",\"\")\n",
    "    word = word.replace(\"â€œ\",\"\")\n",
    "    word = word.replace(\"â€˜\",\"\")\n",
    "    word = word.replace(\"*\",\"\")\n",
    "    if word not in stopwords:\n",
    "        if word not in wordcount:\n",
    "            wordcount[word] = 1\n",
    "        else:\n",
    "            wordcount[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many most common words to print: 20\n",
      "\n",
      "OK. The 20 most common words are as follows\n",
      "\n",
      "thou :  5381\n",
      "thy :  4030\n",
      "shall :  3569\n",
      "thee :  2949\n",
      "good :  2750\n",
      "king :  2711\n",
      "lord :  2703\n",
      "come :  2409\n",
      "enter :  2080\n",
      "hath :  1933\n",
      "love :  1883\n",
      "i'll :  1739\n",
      "like :  1687\n",
      "man :  1653\n",
      "if :  1649\n",
      "know :  1605\n",
      "him :  1594\n",
      "this :  1585\n",
      "he :  1554\n",
      "by :  1535\n"
     ]
    }
   ],
   "source": [
    "n_print = int(input(\"How many most common words to print: \"))\n",
    "print(\"\\nOK. The {} most common words are as follows\\n\".format(n_print))\n",
    "word_counter = collections.Counter(wordcount)\n",
    "for word, count in word_counter.most_common(n_print):\n",
    "    print(word, \": \", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "data = [] \n",
    "for i in sent_tokenize(text): \n",
    "    temp = [] \n",
    "    for j in tokenizer.tokenize(i): \n",
    "        temp.append(j.lower()) \n",
    "  \n",
    "    data.append(temp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus(text):\n",
    "    porter = nltk.PorterStemmer()\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = [] \n",
    "    for i in sent_tokenize(text): \n",
    "        for j in tokenizer.tokenize(i):\n",
    "            words.append(j.lower())\n",
    "    words = np.unique(words)\n",
    "\n",
    "    porter = nltk.PorterStemmer()\n",
    "    words = [porter.stem(t) for t in words]\n",
    "    words = np.unique(words)\n",
    "    \n",
    "    word_to_index = {}\n",
    "    index_to_word = {}\n",
    "    counter = 0;\n",
    "    for w in words:\n",
    "        word_to_index[w] = counter\n",
    "        index_to_word[counter] = w\n",
    "        counter += 1  \n",
    "    return words, word_to_index, index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_co_matrix2(text, words, word_to_index, window=1):\n",
    "    porter = nltk.PorterStemmer()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    corpus_size = len(words)\n",
    "    co_matrix = np.zeros((corpus_size,corpus_size),dtype=int)\n",
    "    for s in sent_tokenize(text): \n",
    "        sent = [] \n",
    "        for w in tokenizer.tokenize(s):        \n",
    "            sent.append(porter.stem(w.lower()))\n",
    "        for i, w in enumerate(sent):\n",
    "            for j in range(max(i-window,0),min(i+window+1,len(sent))):\n",
    "                co_matrix[word_to_index[w],word_to_index[sent[j]]] += 1\n",
    "    return co_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, word_to_index, index_to_word = build_corpus(text)\n",
    "matrix = build_co_matrix2(text, words, word_to_index, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_size = len(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window size  3\n",
      "Cosine similarity between 'king' and 'prince' - CBOW :  0.9207146\n",
      "window size  5\n",
      "Cosine similarity between 'king' and 'prince' - CBOW :  0.9186545\n",
      "window size  7\n",
      "Cosine similarity between 'king' and 'prince' - CBOW :  0.9213121\n",
      "window size  10\n",
      "Cosine similarity between 'king' and 'prince' - CBOW :  0.9482051\n",
      "window size  12\n",
      "Cosine similarity between 'king' and 'prince' - CBOW :  0.92100793\n",
      "window size  15\n",
      "Cosine similarity between 'king' and 'prince' - CBOW :  0.93095136\n",
      "window size  20\n",
      "Cosine similarity between 'king' and 'prince' - CBOW :  0.9585674\n",
      "window size  25\n",
      "Cosine similarity between 'king' and 'prince' - CBOW :  0.95707333\n"
     ]
    }
   ],
   "source": [
    "window_list=[3,5,7,10,12,15,20,25]\n",
    "for i in window_list:\n",
    "    model = gensim.models.Word2Vec(data, min_count = 1,  \n",
    "                              size = 100, window = i)\n",
    "    print(\"window size \",i)\n",
    "    print(\"Cosine similarity between 'king' \" + \n",
    "               \"and 'prince' - CBOW : \", \n",
    "    model.wv.similarity('king', 'prince')) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window size 3\n",
      "Cosine similarity between 'love' and 'life' - CBOW :  0.9184166\n",
      "window size 5\n",
      "Cosine similarity between 'love' and 'life' - CBOW :  0.930352\n",
      "window size 7\n",
      "Cosine similarity between 'love' and 'life' - CBOW :  0.93428457\n",
      "window size 10\n",
      "Cosine similarity between 'love' and 'life' - CBOW :  0.91966665\n",
      "window size 12\n",
      "Cosine similarity between 'love' and 'life' - CBOW :  0.9257948\n",
      "window size 15\n",
      "Cosine similarity between 'love' and 'life' - CBOW :  0.92065775\n",
      "window size 20\n",
      "Cosine similarity between 'love' and 'life' - CBOW :  0.9252828\n",
      "window size 25\n",
      "Cosine similarity between 'love' and 'life' - CBOW :  0.9403358\n"
     ]
    }
   ],
   "source": [
    "window_list=[3,5,7,10,12,15,20,25]\n",
    "for i in window_list:\n",
    "    model = gensim.models.Word2Vec(data, min_count = 1,  \n",
    "                              size = 100, window = i)\n",
    "    print(\"window size\",i)\n",
    "    print(\"Cosine similarity between 'love' \" + \n",
    "               \"and 'life' - CBOW : \", \n",
    "    model.wv.similarity('love', 'life')) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('henry', 0.947674036026001),\n",
       " ('prince', 0.9422085285186768),\n",
       " ('duke', 0.9258636236190796),\n",
       " ('france', 0.9249615669250488),\n",
       " ('queen', 0.9104045033454895)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive='king', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('life', 0.9403358101844788),\n",
       " ('thoughts', 0.9297899007797241),\n",
       " ('honour', 0.9282357692718506),\n",
       " ('self', 0.9237463474273682),\n",
       " ('heart', 0.922876238822937)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive='love', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.2539383e-01,  5.1984650e-01, -1.9127837e-01, -1.7331300e+00,\n",
       "        6.3230805e-02,  2.4035861e-01, -2.7464482e-01, -4.7852714e-02,\n",
       "       -2.1515999e+00, -2.9724622e-01,  1.0151055e+00,  7.2396912e-02,\n",
       "       -2.4122821e-01,  9.1402426e-02, -2.5437903e-01,  9.3016815e-01,\n",
       "        5.0168306e-01,  3.9127040e-01, -5.2005821e-01, -9.6255869e-01,\n",
       "       -5.7866149e-02,  3.4405175e-01, -3.2040128e-01, -9.3581313e-01,\n",
       "        8.4705156e-01,  1.1640203e+00,  1.8569525e+00,  1.2007906e+00,\n",
       "       -1.3896519e-01, -5.4914612e-01,  7.2513305e-02, -5.6529289e-01,\n",
       "        4.9120045e-01,  8.9598727e-01,  6.9172108e-01,  9.1849089e-01,\n",
       "       -5.4376543e-01, -4.9530199e-01,  1.8335420e-03, -5.5325270e-01,\n",
       "        8.0933225e-01,  3.7459135e-02, -6.7267589e-02,  2.7283970e-01,\n",
       "        5.1148504e-01, -5.0786251e-01,  1.4372779e-01, -2.0419238e+00,\n",
       "        1.3239539e+00, -4.6192834e-01, -8.0641955e-02, -4.8716417e-01,\n",
       "       -1.6708932e+00, -4.0202945e-02,  3.9717317e-01, -2.4634288e-01,\n",
       "       -1.3287200e+00,  1.9429325e+00,  8.5127580e-01, -4.7978318e-01,\n",
       "        8.9169484e-01,  6.4849377e-01, -5.9519076e-01,  1.1950958e+00,\n",
       "       -2.5992972e-01,  1.2759267e-01, -9.6697721e-04, -2.7270320e-01,\n",
       "        1.6318996e+00,  7.0228130e-01, -9.1902739e-01,  4.1549534e-01,\n",
       "       -8.8677585e-01,  1.8559426e-02, -1.2265118e+00,  1.0927229e+00,\n",
       "       -3.9426759e-01, -2.7159193e-01, -5.2588999e-01, -1.3504758e-01,\n",
       "        1.2206048e+00,  9.8349053e-01, -1.0259092e+00,  1.3361445e-01,\n",
       "       -1.3459085e-01, -3.9931822e-01, -1.2823507e+00,  1.2065399e+00,\n",
       "        3.5512444e-01,  2.7993119e-01, -7.3817647e-01, -1.2548651e+00,\n",
       "        1.4733170e+00,  4.7684142e-01,  3.8429654e-01, -9.0507954e-01,\n",
       "        1.3814509e+00,  4.5814398e-01,  1.2145730e+00,  7.3251313e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
